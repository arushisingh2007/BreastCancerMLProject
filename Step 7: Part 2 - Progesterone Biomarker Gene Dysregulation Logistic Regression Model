X = df_gene.drop(['ER Status', 'PR Status', 'HER2 Final Status'], axis=1)  # Features
y = df_gene['PR Status']  # Target variable

# Scaling and imputation
imputer = SimpleImputer()
scaler = StandardScaler()

# X_imputed = imputer.fit_transform(X)
# X_scaled = scaler.fit_transform(X_imputed)

# Perform feature selection using LinearSVC with L1 penalty
lsvc = LinearSVC(C=0.05, penalty="l1", dual=False)
# lsvc.fit(X_scaled, y)

# Select features based on the model
model = SelectFromModel(lsvc, prefit= False)
# X_selected = model.transform(X_scaled)

# Define your logistic regression model
logreg_model = LogisticRegressionCV(penalty='elasticnet',
                                    Cs= [0.1, 1.0, 10.0], l1_ratios=[0.5, 0.7, 0.9, 0.97],
                                    solver='saga', max_iter=int(1e5), tol=1e-5)

# Create a pipeline for feature selection and model training
pipeline = Pipeline([('inputer', imputer), ('scaler', scaler), ('selector', model), ('estimator', logreg_model)])

# Define your cross-validation strategy
skf1 = StratifiedKFold(n_splits= 10, shuffle=True, random_state=42)

scores = cross_val_score(pipeline, X, y, cv=skf1)

predicted_probabilities = cross_val_predict(pipeline, X, y, cv=skf1, method='predict_proba')

# Compute ROC AUC score
roc_auc = roc_auc_score(y, predicted_probabilities[:, 1])


print("Cross-Validation Scores:", scores)
print("Mean Cross-Validation Score:", np.mean(scores))
print("ROC AUC Score:", roc_auc)

# Fit the model on the entire training data
# imputer = SimpleImputer()
label_encoder = LabelEncoder()
y_encoded = label_encoder.fit_transform(y)

from sklearn.metrics import PrecisionRecallDisplay
PrecisionRecallDisplay.from_predictions(y_encoded, predicted_probabilities[:, 1])
plt.show()
print("Cross-Validation Scores:", scores)
print("Mean Cross-Validation Score:", np.mean(scores))
print("ROC AUC Score:", roc_auc)
logreg_coef = pipeline.named_steps['estimator'].coef_[0]

# Printing genes and their corresponding coefficients in a vertical format
positive_genes = []
positive_coefs = []
negative_genes = []
negative_coefs = []

for gene, coef in zip(X.columns, logreg_coef):
    if coef > 0:
        positive_genes.append(gene)
        positive_coefs.append(coef)
    else:
        negative_genes.append(gene)
        negative_coefs.append(coef)

# Define the gene you want to exclude
gene_to_exclude = 'Complete TCGA ID'

# Remove the gene and its corresponding coefficient from the lists
if gene_to_exclude in positive_genes:
    index = positive_genes.index(gene_to_exclude)
    del positive_genes[index]
    del positive_coefs[index]
elif gene_to_exclude in negative_genes:
    index = negative_genes.index(gene_to_exclude)
    del negative_genes[index]
    del negative_coefs[index]

plt.figure(figsize=(10, 8))

# Plotting positive coefficients
plt.barh(positive_genes, positive_coefs, color='blue', label='Positive Coefficients for PR+ Breast Cancer')

# Plotting negative coefficients
plt.barh(negative_genes, negative_coefs, color='red', label='Negative Coefficients for PR- Breast Cancer')
plt.yticks(fontsize=16)  # Y-axis labels (genes)
plt.xticks(fontsize=12)  # X-axis labels (coefficient values)
plt.xlabel('Coefficient Value')
plt.ylabel('Gene')
plt.title('Coefficients of Genes')
plt.legend()
# scaler = StandardScaler()

# X_imputed = imputer.fit_transform(X)
# X_scaled = scaler.fit_transform(X_imputed)
pipeline.fit(X, y)
